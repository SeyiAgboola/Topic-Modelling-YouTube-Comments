{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modelling YouTube Comments.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u9J9-dBPWtd"
      },
      "source": [
        "# Topic Modelling YouTube Comments\r\n",
        "\r\n",
        "In this notebook, I am going to perform topic modelling on YouTube Comments I've extracted previously. There are many Topic Modelling articles available that work through each stage in enough detail. Still I believe there's room for showing beginners how to get the best performance out of the topic modelling code.\r\n",
        "\r\n",
        "When performing topic modelling you need to identify:\r\n",
        "1. The Document\r\n",
        "2. The Paragraphs within the document\r\n",
        "3. The approach \r\n",
        "4. The assumptions\r\n",
        "\r\n",
        "In this example, I will be using Cyberpunk 2077 YouTube trailer comment sections as my documents.\r\n",
        "\r\n",
        "1. The document - Whole comment section\r\n",
        "2. The paragraphs - Each individual comment\r\n",
        "3. The approach - LDA (and maybe NNMF after)\r\n",
        "4. The assumptions - Similar words = Similar topics/ Re-occuring groups of words = similar topic\r\n",
        "\r\n",
        "I will be following the steps outlined in [this article](https://stackabuse.com/python-for-nlp-topic-modeling/) as I felt everything was thoroughly explained step by step.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "XQ5ShHojPNbU",
        "outputId": "42089098-b62c-4fbb-ad58-ada8ec0cd533"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "dataset = pd.read_csv(\"/content/4yQ--nrwy5w_comments.csv\")\r\n",
        "dataset.columns = [\"Comment\", \"Comment ID\", \"Reply Count\", \"Like Count\", \"Publish Date\"]\r\n",
        "display(youtube_dataset.head())\r\n",
        "print(youtube_dataset.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>Reply Count</th>\n",
              "      <th>Like Count</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I like how theres quests in game with song nam...</td>\n",
              "      <td>UgzBfnjpNP8sTyVuEh54AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-02T13:18:19Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is one of the best game trailers ever mad...</td>\n",
              "      <td>UgxmWkk3_vhYmnxOYzl4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-02T12:37:41Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No matter what people say about this game and ...</td>\n",
              "      <td>UgxwIEXrUcnwg7s3JOl4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-02T11:54:51Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It gives me chills every time I watch this. Ik...</td>\n",
              "      <td>UgwAaP00la2JB6vrCOZ4AaABAg</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-02T11:13:04Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>POLSKA GRA I NIE MA POLAKOW XDDDXDDD</td>\n",
              "      <td>UgxIKIqITUIheHzS-cZ4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-02T11:12:49Z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Comment  ...          Publish Date\n",
              "0  I like how theres quests in game with song nam...  ...  2021-01-02T13:18:19Z\n",
              "1  This is one of the best game trailers ever mad...  ...  2021-01-02T12:37:41Z\n",
              "2  No matter what people say about this game and ...  ...  2021-01-02T11:54:51Z\n",
              "3  It gives me chills every time I watch this. Ik...  ...  2021-01-02T11:13:04Z\n",
              "4               POLSKA GRA I NIE MA POLAKOW XDDDXDDD  ...  2021-01-02T11:12:49Z\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(24087, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54RNK0iHUBCH"
      },
      "source": [
        "# Looking at the text data\r\n",
        "\r\n",
        "It's always important to look at the raw text data to check for things that might not get picked up by the machine which is definitely the case with Social Media.\r\n",
        "\r\n",
        "The use of slang, typos and poor grammar can hinder the effectiveness of NLP models significantly, so we will to do some significant cleaning before we do any technical analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXYK4oofS9SR",
        "outputId": "ac0b8431-d4eb-4744-f527-3cf410b4a85a"
      },
      "source": [
        "print(dataset['Comment'][351])\r\n",
        "print(\" \")\r\n",
        "print(dataset['Comment'][1200])\r\n",
        "print(\" \")\r\n",
        "print(dataset['Comment'][45])\r\n",
        "print(\" \")\r\n",
        "print(dataset['Comment'][1666])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Absolutely beuatiful... can't wait to get my hands on this game.\n",
            " \n",
            "Is it me or it doesnt look sooo beautiful as they have been showing us?\n",
            " \n",
            "I say we start a petition to see to it that no cdpr developer receives their bonus.\n",
            " \n",
            "This is the only game that makes me think about getting a new gen console\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vlsjs8BY2xs"
      },
      "source": [
        "You can clean your text data without creating a function but I believe it is more thorough to put as much of the preprocessing in functions so you can tailor it to the different text data you'll be working with.\r\n",
        "\r\n",
        "In this case we are cleaning by:\r\n",
        "* Lowercasing the text\r\n",
        "* Removing numbers\r\n",
        "* Stripping out white text\r\n",
        "* Removing punctuation\r\n",
        "* (Optional) - Removing topic specific stopwords. If the document is about Cyberpunk 2077 then CDPR will most likely appear consistently, whether this is useful in your topic modelling is up to you.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LwwFLidPUjUT",
        "outputId": "a195bae0-a63c-4950-db90-8f10bfe969b3"
      },
      "source": [
        "def clean_text(text):\r\n",
        "  text = text.lower().replace(\"'\",\"\").replace('[^\\w\\s]', ' ').replace(\" \\d+\", \" \").strip()\r\n",
        "  return text\r\n",
        "\r\n",
        "\r\n",
        "sample = dataset['Comment'][351]\r\n",
        "clean_text(sample)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'absolutely beuatiful... cant wait to get my hands on this game.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "cUv-SaSoZ1sZ",
        "outputId": "c914db80-5dd0-4bdd-a31f-a31e44cd3bfd"
      },
      "source": [
        "dataset['Clean Comment'] = dataset['Comment'].apply(clean_text)\r\n",
        "dataset.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>Reply Count</th>\n",
              "      <th>Like Count</th>\n",
              "      <th>Publish Date</th>\n",
              "      <th>Clean Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>so that was a fucking lie</td>\n",
              "      <td>Ugz2OiOZa8EZbPRPfHt4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-01T05:51:31Z</td>\n",
              "      <td>so that was a fucking lie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1:13 bro why are you going so slow... why you ...</td>\n",
              "      <td>UgylPC2tsFYHrGFd-UN4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-12-31T03:03:45Z</td>\n",
              "      <td>1:13 bro why are you going so slow... why you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LLLLLIIIIEEEESSSSSS!!!!!!!!</td>\n",
              "      <td>UgzwwVVxRnmPkw8HEvl4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-12-29T12:27:39Z</td>\n",
              "      <td>llllliiiieeeessssss!!!!!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nice relaxing scam</td>\n",
              "      <td>UgwFVxwEn2tpceqFm_94AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-12-29T09:59:42Z</td>\n",
              "      <td>nice relaxing scam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just coming back here to see how much we were ...</td>\n",
              "      <td>UgyJLTdjbqX464mJ8xZ4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-12-29T06:28:09Z</td>\n",
              "      <td>just coming back here to see how much we were ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Comment  ...                                      Clean Comment\n",
              "0                          so that was a fucking lie  ...                          so that was a fucking lie\n",
              "1  1:13 bro why are you going so slow... why you ...  ...  1:13 bro why are you going so slow... why you ...\n",
              "2                        LLLLLIIIIEEEESSSSSS!!!!!!!!  ...                        llllliiiieeeessssss!!!!!!!!\n",
              "3                                 Nice relaxing scam  ...                                 nice relaxing scam\n",
              "4  Just coming back here to see how much we were ...  ...  just coming back here to see how much we were ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px7CdG6ju4r4"
      },
      "source": [
        "# Advanced Optimisation of Text Data \r\n",
        "## Skip section until you have ran LDA model first time round.\r\n",
        "\r\n",
        "There is so much more you can do if the text data is very dirty (typos, grammar, memes) to make the data optimised for topic modelling including:\r\n",
        "\r\n",
        "* Word or Character limit\r\n",
        "* Removal Topic specific stopwords\r\n",
        "* Language translation\r\n",
        "* Lemming\r\n",
        "* Stemming\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOm0Fh0ywVLU",
        "outputId": "15ced8bd-6e5c-44b0-b57a-542f0be7273e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "#stop_words_list = stopwords.words('english') + ['though', 'game', 'games']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9ID4N4QwtEU",
        "outputId": "2119cdef-b0f8-4fd5-e026-db30c106e8fc"
      },
      "source": [
        "# initiate stopwords from nltk\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "print(len(stop_words))\r\n",
        "# add additional missing terms\r\n",
        "\r\n",
        "stop_words.extend(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m','n','o','p','q','r','s','t', 'u', 'v', 'w', 'x', 'y', 'z', \"about\", \"across\", \"after\", \"all\", \"also\", \"an\", \"and\", \"another\", \"added\",\r\n",
        "\"any\", \"are\", \"as\", \"at\", \"basically\", \"be\", \"because\", 'become', \"been\", \"before\", \"being\", \"between\",\"both\", \"but\", \"by\",\"came\",\"can\",\"come\",\"could\",\"did\",\"do\",\"does\",\"each\",\"else\",\"every\",\"either\",\"especially\", \"for\",\"from\",\"get\",\"given\",\"gets\",\r\n",
        "'give','gives',\"got\",\"goes\",\"had\",\"has\",\"have\",\"he\",\"her\",\"here\",\"him\",\"himself\",\"his\",\"how\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"just\",\"lands\",\"like\",\"make\",\"making\", \"made\", \"many\",\"may\",\"me\",\"might\",\"more\",\"most\",\"much\",\"must\",\"my\",\"never\",\"provide\", \r\n",
        "\"provides\", \"perhaps\",\"no\",\"now\",\"of\",\"on\",\"only\",\"or\",\"other\", \"our\",\"out\",\"over\",\"re\",\"said\",\"same\",\"see\",\"should\",\"since\",\"so\",\"some\",\"still\",\"such\",\"seeing\", \"see\", \"take\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\r\n",
        "\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"up\",\"use\",\"using\",\"used\", \"underway\", \"very\",\"want\",\"was\",\"way\",\"we\",\"well\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"whilst\",\"who\",\"will\",\"with\",\"would\",\"you\",\"your\", \r\n",
        "'etc', 'via', 'eg', 'game', 'games' 'like']) \r\n",
        "\r\n",
        "# remove stopwords\r\n",
        "print(len(stop_words))\r\n",
        "#df[1] = df[1].apply(lambda x: [item for item in x if item not in stop_words])\r\n",
        "\r\n",
        "#display(df.head(10))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q9USnbrwt65",
        "outputId": "253b9f0a-8928-4180-9140-0682b603dfc8"
      },
      "source": [
        "def superclean(text):\r\n",
        "  #tokens = text.split(\" \")\r\n",
        "  text = text.lower().replace(\"'\",\"\").replace('[^\\w\\s]', ' ').replace(\" \\d+\", \" \").strip()\r\n",
        "  tokens = nltk.word_tokenize(text)\r\n",
        "  stop_tokens = [item for item in tokens if item not in stop_words]\r\n",
        "  new_text = ' '.join(stop_tokens)\r\n",
        "  return new_text\r\n",
        "print(youtube_dataset['Comment'][45])\r\n",
        "print(\"------\")\r\n",
        "print(superclean(youtube_dataset['Comment'][45]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It is a shame to have such errors in a game that has only been released in 8 years, and it is disrespectful to people who expect the game\r\n",
            "And as someone who does not like FPS games, I am sure that if there were camera angles such as GTA, he would appeal to more masses\n",
            "------\n",
            "shame errors released 8 years , disrespectful people expect someone fps games , sure camera angles gta , appeal masses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "W18P-lR_YCUC",
        "outputId": "30c52734-3a36-48be-ff2c-31994678a636"
      },
      "source": [
        "dataset['Clean Comment'] = dataset['Comment'].apply(superclean)\r\n",
        "dataset.head()\r\n",
        "dataset[\"char_count\"] = dataset['Clean Comment'].apply(len)\r\n",
        "df = dataset.drop(dataset[dataset['char_count'] < 50].index)\r\n",
        "#df = df.drop(df[df.score < 50].index)\r\n",
        "#df = df.drop(df[(df.score < 50) & (df.score > 20)].index)\r\n",
        "print(df.shape)\r\n",
        "print(dataset.shape)\r\n",
        "display(df.head())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1348, 3)\n",
            "(1928, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Clean Comment</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What this is telling me is I should get one of...</td>\n",
              "      <td>telling one new consoles play pc . dont chuggi...</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Honestly, I'd be fine if it was delayed until ...</td>\n",
              "      <td>honestly , id fine delayed new consoles releas...</td>\n",
              "      <td>285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Feel like they buried the lede where the sourc...</td>\n",
              "      <td>feel buried lede source mentioned cdpr worked ...</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To anyone mildly worried that this means that ...</td>\n",
              "      <td>anyone mildly worried means wont release ps4/x...</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I'm at that point where I'm like, do I play on...</td>\n",
              "      <td>im point im , play ps4 , wait play ps5 . even ...</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Comment  ... char_count\n",
              "0  What this is telling me is I should get one of...  ...         96\n",
              "1  Honestly, I'd be fine if it was delayed until ...  ...        285\n",
              "2  Feel like they buried the lede where the sourc...  ...         63\n",
              "3  To anyone mildly worried that this means that ...  ...        145\n",
              "4  I'm at that point where I'm like, do I play on...  ...        151\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVD_7omzanwH"
      },
      "source": [
        "# Create your Document Term Matrix\r\n",
        "\r\n",
        "You need a Document Term Matrix of some sort to do the topic modelling calculations. \r\n",
        "\r\n",
        "I've seen a lot of topic modelling approaches that skip customising their Vectoriser. We won't because never forget, garbage in, garbage out.\r\n",
        "\r\n",
        "Before converting our words into numeric values, we will:#\r\n",
        "\r\n",
        "* Only include those words that appear in less than 80% of the document (max_df=0.8)\r\n",
        "* Only include those words that ppear in at least 2 documents\r\n",
        "* Remove english stopwords (even if you've removed stopwords before, do it again as different models you've used previously might have missed out some words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLYexp-vaEPL"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "#Only include those words that appear in less than 80% of the document (max_df=0.8)\r\n",
        "#Only include those words that ppear in at least 2 documents\r\n",
        "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\r\n",
        "#.values Only the values in the DataFrame will be returned, the axes labels will be removed.\r\n",
        "#The astype(‘U’) is telling numpy to convert the data to Unicode (essentially a string in python 3).\r\n",
        "doc_term_matrix = count_vect.fit_transform(dataset['Clean Comment'].values.astype('U'))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb4_tzmRcDci",
        "outputId": "a8a74cab-177f-44d2-983e-108d5b564761"
      },
      "source": [
        "doc_term_matrix\r\n",
        "#Each of 16236 documents(comments) is represented as 9651 dimensional vector, which means that our vocabulary has 9651 words."
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1348x2719 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 23833 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TROTu_Q2fGK3"
      },
      "source": [
        "# Run your LDA Model\r\n",
        "\r\n",
        "This is quite a basic implementation of LDA without the [extensive list of parameters](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) you can edit. So I added a more advanced version underneath as a comparison.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBS_05tAcKXP",
        "outputId": "b970ccb7-a220-4e79-d124-86105caf5bf5"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\r\n",
        "#n_components = num. of topics\r\n",
        "#random_state = Is just random\r\n",
        "LDA = LatentDirichletAllocation(n_components=10, random_state=42)\r\n",
        "LDA.fit(doc_term_matrix)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs8aj5iVfdwD",
        "outputId": "24490d4e-e83b-465d-b583-a5d30c69724b"
      },
      "source": [
        "#Thanks fellow coder - \r\n",
        "#https://stackoverflow.com/questions/61373994/how-to-specify-random-state-in-lda-model-for-topic-modelling\r\n",
        "LDA_Advanced = LatentDirichletAllocation(n_components=10,        \r\n",
        "                                  max_iter=10,               \r\n",
        "                                  learning_method='online',   \r\n",
        "                                  random_state=100,          \r\n",
        "                                  batch_size=128,            \r\n",
        "                                  evaluate_every = -1,       \r\n",
        "                                  n_jobs = -1 )\r\n",
        "\r\n",
        "LDA_Advanced.fit(doc_term_matrix)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='online', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=10, n_jobs=-1,\n",
              "                          perp_tol=0.1, random_state=100, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PixPVaHPex_X",
        "outputId": "7c1ad936-904c-415d-bc3c-fce983abba9c"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  #randomly fetch 10 word ids using the get_feature_names() method\r\n",
        "    random_id = random.randint(0,len(count_vect.get_feature_names()))\r\n",
        "    print(count_vect.get_feature_names()[random_id])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mountain\n",
            "contradicts\n",
            "north\n",
            "microsoft\n",
            "profile\n",
            "properly\n",
            "companies\n",
            "enormous\n",
            "janky\n",
            "2500k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M3kOzbch4Ro"
      },
      "source": [
        "# Print out the Top 10 words for Topic 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYed-UTNgVVp",
        "outputId": "24731aec-8dc5-4d8f-f919-c37e4b10dfaa"
      },
      "source": [
        "#Use components_ to get the first topic at index 0\r\n",
        "first_topic = LDA.components_[0]\r\n",
        "#Out of 14546 probabilities for each word for topic 1, sort the indexes according to probability values - argsort().\r\n",
        "#Return last 10 indexes [-10]\r\n",
        "top_topic_words = first_topic.argsort()[-10:]\r\n",
        "\r\n",
        "for i in top_topic_words:\r\n",
        "    print(count_vect.get_feature_names()[i])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "consoles\n",
            "play\n",
            "run\n",
            "ps5\n",
            "ps4\n",
            "lot\n",
            "think\n",
            "people\n",
            "pc\n",
            "games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah-7YuGgjIaT"
      },
      "source": [
        "# First Attempt at Top Topics And Analysis\r\n",
        "\r\n",
        "* Topic 0: December 10th delay\r\n",
        "* Topic 1: undistinguishable - Foreign language and stopwords\r\n",
        "* Topic 2: Cars look great\r\n",
        "* Topic 3: Positive reception, in awe\r\n",
        "* Topic 4: Can't wait to play on next gen or PC\r\n",
        "* Topic 5: Technical Capabilities of running 4k and 1080p on consoles\r\n",
        "* Topic 6: Undistinguishable - Very mixed\r\n",
        "* Topic 7: Looks good\r\n",
        "* Topic 8: Undistinghuishable - Very mixed\r\n",
        "* Topic 9: The look of night city\r\n",
        "\r\n",
        "There are so clear topics but there is also a lot of overlap of positive sentiment from \"wow+omg\" to \"good+great+love\". We can maybe reduce the overlap by having less topics.\r\n",
        "\r\n",
        "Non-english words is a poor oversight by me and should be factored into the clean_text function.\r\n",
        "\r\n",
        "There's scope for adding new stopwords such as \"like, game, im, whats, video\". Even https is an interesting removal as it refers to external links to other websites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBA7o__UhvNj",
        "outputId": "83790e88-5992-477c-e3d8-2d47a42f2eb0"
      },
      "source": [
        "for i,topic in enumerate(LDA.components_):\r\n",
        "    print(f'Top 10 words for topic #{i}:')\r\n",
        "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\r\n",
        "    print('\\n')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words for topic #0:\n",
            "['10', 'just', '10th', 'wait', 'year', 'release', 'delay', 'december', 'gen', 'game']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1:\n",
            "['look', 'на', '3rd', 'что', 'не', 'https', 'im', 'game', 'just', 'person']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2:\n",
            "['video', 'look', 'fucking', 'love', 'car', 'really', 'im', 'just', 'game', 'like']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3:\n",
            "['got', 'just', 'good', 'wow', 'awesome', 'shit', 'dont', 'like', 'looks', 'game']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4:\n",
            "['gen', 'pc', 'im', 'ps5', 'play', 'xbox', 'wait', 'game', 'series', 'looks']\n",
            "\n",
            "\n",
            "Top 10 words for topic #5:\n",
            "['gameplay', 'dont', 'run', 'consoles', '1080p', 'video', '4k', 'ps4', 'xbox', 'game']\n",
            "\n",
            "\n",
            "Top 10 words for topic #6:\n",
            "['15', 'like', 'know', '60', 'whats', 'hollie', 'xbox', 'song', '30', 'fps']\n",
            "\n",
            "\n",
            "Top 10 words for topic #7:\n",
            "['really', 'amazing', 'bad', 'people', 'gen', 'game', 'look', 'good', 'like', 'looks']\n",
            "\n",
            "\n",
            "Top 10 words for topic #8:\n",
            "['god', 'gta', '50', 'look', 'game', 'reeves', 'looks', 'music', 'like', 'keanu']\n",
            "\n",
            "\n",
            "Top 10 words for topic #9:\n",
            "['pop', 'world', 'just', 'people', 'like', 'npcs', 'streets', 'night', 'looks', 'city']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o0s8qbRljTK"
      },
      "source": [
        "The topics with the advanced LDA model are completely different.\r\n",
        "\r\n",
        "* Topic 0: 1080p or 4k on playstation\r\n",
        "* Topic 1: Current gen will run this at 30fps\r\n",
        "* Topic 2: Console versions\r\n",
        "* Topic 3: Keanu Reeves is awesome\r\n",
        "* Topic 4: Game should be delayed something about graphics\r\n",
        "* Topic 5: Looks great on PS5\r\n",
        "* Topic 6: Undistinguishable -  Mixture of topics\r\n",
        "* Topic 7: The gameplay looks good\r\n",
        "* Topic 8: Weird pop ins with Combat and Car mechanics\r\n",
        "* Topic 9: Ray tracing isn't going to make a difference.\r\n",
        "\r\n",
        "The LDA Advanced might have just performed better with a more unique set of words but some of the issues are still present.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDShW1p8i0kC",
        "outputId": "bc2608b7-6f4b-4314-f6fa-a7c537dd87d9"
      },
      "source": [
        "for i,topic in enumerate(LDA_Advanced.components_):\r\n",
        "    print(f'Top 10 words for topic #{i}:')\r\n",
        "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\r\n",
        "    print('\\n')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words for topic #0:\n",
            "['upload', 'screen', 'que', 'yes', 'boring', 'playstation', 'excited', '1080p', 'video', '4k']\n",
            "\n",
            "\n",
            "Top 10 words for topic #1:\n",
            "['running', '30fps', 'current', 'character', 'series', 'year', 'fucking', 'consoles', '10', 'gen']\n",
            "\n",
            "\n",
            "Top 10 words for topic #2:\n",
            "['version', 'just', 'consoles', 'new', 'console', 'delay', 'ps4', 'pc', 'hope', 'xbox']\n",
            "\n",
            "\n",
            "Top 10 words for topic #3:\n",
            "['johnny', 'reeves', 'что', 'downgrade', 'на', 'не', '50', 'awesome', 'song', 'keanu']\n",
            "\n",
            "\n",
            "Top 10 words for topic #4:\n",
            "['delayed', '10th', 'graphics', 'lol', 'december', 'cyberpunk', '2077', 'game', 'looks', 'like']\n",
            "\n",
            "\n",
            "Top 10 words for topic #5:\n",
            "['version', 'just', 'great', 'fps', 'play', 'ps5', 'release', 'series', 'wait', 'game']\n",
            "\n",
            "\n",
            "Top 10 words for topic #6:\n",
            "['god', 'wtf', 'looks', 'time', 'shit', 'game', 'waiting', 'like', '60fps', 'nice']\n",
            "\n",
            "\n",
            "Top 10 words for topic #7:\n",
            "['just', 'city', 'dont', 'people', 'gameplay', 'im', 'good', 'like', 'looks', 'game']\n",
            "\n",
            "\n",
            "Top 10 words for topic #8:\n",
            "['driving', 'pop', 'weird', 'ass', 'drops', 'far', 'whats', 'fuck', 'combat', 'car']\n",
            "\n",
            "\n",
            "Top 10 words for topic #9:\n",
            "['tracing', 'thanks', 'ray', 'work', 'mean', 'life', 'bugs', 'isnt', 'coming', 'difference']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnNTvjLjnqz_"
      },
      "source": [
        "# Add Topic Column to dataframe\r\n",
        "\r\n",
        "This is a nice way to look how aligned the topic allocation is to each comment. Short 1-2 worded comments are obviously going to be completely incorrect but I have seen a lot of very accurate Topic allocations when compared to what I assumed the topics to be about.\r\n",
        "\r\n",
        "So that's another thing I need to do for the second run. Remove short comments which are shorter than one first so maybe 10 words or 50 characters?\r\n",
        "\r\n",
        "I think at this point I need to reveal what the video is about.... \r\n",
        "\r\n",
        "\r\n",
        "[Cyberpunk 2077 — Night City Wire Special: Xbox One X and Xbox Series X footage](https://www.youtube.com/watch?v=4yQ--nrwy5w&ab_channel=Cyberpunk2077)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "8lF7CFkYnlHw",
        "outputId": "7ddd5da8-cf21-42cb-b3bd-fde0373ccdc9"
      },
      "source": [
        "topic_values = LDA_Advanced.transform(doc_term_matrix)\r\n",
        "topic_values.shape\r\n",
        "dataset['Topic'] = topic_values.argmax(axis=1)\r\n",
        "dataset[16000:16010]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>Reply Count</th>\n",
              "      <th>Like Count</th>\n",
              "      <th>Publish Date</th>\n",
              "      <th>Clean Comment</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16000</th>\n",
              "      <td>They look like angels</td>\n",
              "      <td>Ugy5voH_HW9F4E_mfmR4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:13:07Z</td>\n",
              "      <td>they look like angels</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16001</th>\n",
              "      <td>I have reached levels of hype for this game th...</td>\n",
              "      <td>UgzDew78mKSh8ERMAC14AaABAg</td>\n",
              "      <td>2</td>\n",
              "      <td>106</td>\n",
              "      <td>2020-11-17T17:13:06Z</td>\n",
              "      <td>i have reached levels of hype for this game th...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16002</th>\n",
              "      <td>The people in the chat were going ballistic be...</td>\n",
              "      <td>UgxMalIws8N4G5CS4fp4AaABAg</td>\n",
              "      <td>26</td>\n",
              "      <td>518</td>\n",
              "      <td>2020-11-17T21:57:03Z</td>\n",
              "      <td>the people in the chat were going ballistic be...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16003</th>\n",
              "      <td>Looks unplayable on current gen consoles.</td>\n",
              "      <td>UgwF_ZaOev1Izbw6zFV4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:13:04Z</td>\n",
              "      <td>looks unplayable on current gen consoles.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16004</th>\n",
              "      <td>Hollie Bennett is really cute, she has an Inst...</td>\n",
              "      <td>UgzrEKsm76OVQikx6XB4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:13:03Z</td>\n",
              "      <td>hollie bennett is really cute, she has an inst...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16005</th>\n",
              "      <td>Who else can't wait for their pre-order?</td>\n",
              "      <td>Ugy2BckZ4Q5OGoXFG6d4AaABAg</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-11-17T17:13:00Z</td>\n",
              "      <td>who else cant wait for their pre-order?</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16006</th>\n",
              "      <td>12 Tflops = 30fps console plebs lmao</td>\n",
              "      <td>Ugzb0jdME2n0WTFBGq94AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:13:00Z</td>\n",
              "      <td>12 tflops = 30fps console plebs lmao</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16007</th>\n",
              "      <td>Wow, this looks awesome!</td>\n",
              "      <td>UgwFJPYFwmfojerNcW94AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:12:59Z</td>\n",
              "      <td>wow, this looks awesome!</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16008</th>\n",
              "      <td>Xbox? What is this?</td>\n",
              "      <td>Ugw3T34PJjXpz1ptevN4AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:12:59Z</td>\n",
              "      <td>xbox? what is this?</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16009</th>\n",
              "      <td>I just hope it works on my ps4</td>\n",
              "      <td>UgypkOZR-r3oRIQHv454AaABAg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-17T17:12:53Z</td>\n",
              "      <td>i just hope it works on my ps4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Comment  ... Topic\n",
              "16000                              They look like angels  ...     4\n",
              "16001  I have reached levels of hype for this game th...  ...     7\n",
              "16002  The people in the chat were going ballistic be...  ...     7\n",
              "16003          Looks unplayable on current gen consoles.  ...     1\n",
              "16004  Hollie Bennett is really cute, she has an Inst...  ...     7\n",
              "16005           Who else can't wait for their pre-order?  ...     7\n",
              "16006               12 Tflops = 30fps console plebs lmao  ...     1\n",
              "16007                           Wow, this looks awesome!  ...     7\n",
              "16008                                Xbox? What is this?  ...     2\n",
              "16009                     I just hope it works on my ps4  ...     2\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}